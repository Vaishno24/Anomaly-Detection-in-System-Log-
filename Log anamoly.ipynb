{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9c97959",
   "metadata": {},
   "source": [
    "## <center> LOG ANAMOLY DETECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e94713",
   "metadata": {},
   "source": [
    "## log parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d813772d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description : This file implements the Drain algorithm for log parsing\n",
    "Author      : LogPAI team\n",
    "License     : MIT\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class Logcluster:\n",
    "    def __init__(self, logTemplate='', logIDL=None):\n",
    "        self.logTemplate = logTemplate\n",
    "        if logIDL is None:\n",
    "            logIDL = []\n",
    "        self.logIDL = logIDL\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, childD=None, depth=0, digitOrtoken=None):\n",
    "        if childD is None:\n",
    "            childD = dict()\n",
    "        self.childD = childD\n",
    "        self.depth = depth\n",
    "        self.digitOrtoken = digitOrtoken\n",
    "\n",
    "\n",
    "class LogParser:\n",
    "    def __init__(self, log_format, indir='./', outdir='./result/', depth=4, st=0.4, \n",
    "                 maxChild=100, rex=[], keep_para=True):\n",
    "        \"\"\"\n",
    "        Attributes\n",
    "        ----------\n",
    "            rex : regular expressions used in preprocessing (step1)\n",
    "            path : the input path stores the input log file name\n",
    "            depth : depth of all leaf nodes\n",
    "            st : similarity threshold\n",
    "            maxChild : max number of children of an internal node\n",
    "            logName : the name of the input file containing raw log messages\n",
    "            savePath : the output path stores the file containing structured logs\n",
    "        \"\"\"\n",
    "        self.path = indir\n",
    "        self.depth = depth - 2\n",
    "        self.st = st\n",
    "        self.maxChild = maxChild\n",
    "        self.logName = None\n",
    "        self.savePath = outdir\n",
    "        self.df_log = None\n",
    "        self.log_format = log_format\n",
    "        self.rex = rex\n",
    "        self.keep_para = keep_para\n",
    "\n",
    "    def hasNumbers(self, s):\n",
    "        return any(char.isdigit() for char in s)\n",
    "\n",
    "    def treeSearch(self, rn, seq):\n",
    "        retLogClust = None\n",
    "\n",
    "        seqLen = len(seq)\n",
    "        if seqLen not in rn.childD:\n",
    "            return retLogClust\n",
    "\n",
    "        parentn = rn.childD[seqLen]\n",
    "\n",
    "        currentDepth = 1\n",
    "        for token in seq:\n",
    "            if currentDepth >= self.depth or currentDepth > seqLen:\n",
    "                break\n",
    "\n",
    "            if token in parentn.childD:\n",
    "                parentn = parentn.childD[token]\n",
    "            elif '<*>' in parentn.childD:\n",
    "                parentn = parentn.childD['<*>']\n",
    "            else:\n",
    "                return retLogClust\n",
    "            currentDepth += 1\n",
    "\n",
    "        logClustL = parentn.childD\n",
    "\n",
    "        retLogClust = self.fastMatch(logClustL, seq)\n",
    "\n",
    "        return retLogClust\n",
    "\n",
    "    def addSeqToPrefixTree(self, rn, logClust):\n",
    "        seqLen = len(logClust.logTemplate)\n",
    "        if seqLen not in rn.childD:\n",
    "            firtLayerNode = Node(depth=1, digitOrtoken=seqLen)\n",
    "            rn.childD[seqLen] = firtLayerNode\n",
    "        else:\n",
    "            firtLayerNode = rn.childD[seqLen]\n",
    "\n",
    "        parentn = firtLayerNode\n",
    "\n",
    "        currentDepth = 1\n",
    "        for token in logClust.logTemplate:\n",
    "\n",
    "            #Add current log cluster to the leaf node\n",
    "            if currentDepth >= self.depth or currentDepth > seqLen:\n",
    "                if len(parentn.childD) == 0:\n",
    "                    parentn.childD = [logClust]\n",
    "                else:\n",
    "                    parentn.childD.append(logClust)\n",
    "                break\n",
    "\n",
    "            #If token not matched in this layer of existing tree. \n",
    "            if token not in parentn.childD:\n",
    "                if not self.hasNumbers(token):\n",
    "                    if '<*>' in parentn.childD:\n",
    "                        if len(parentn.childD) < self.maxChild:\n",
    "                            newNode = Node(depth=currentDepth + 1, digitOrtoken=token)\n",
    "                            parentn.childD[token] = newNode\n",
    "                            parentn = newNode\n",
    "                        else:\n",
    "                            parentn = parentn.childD['<*>']\n",
    "                    else:\n",
    "                        if len(parentn.childD)+1 < self.maxChild:\n",
    "                            newNode = Node(depth=currentDepth+1, digitOrtoken=token)\n",
    "                            parentn.childD[token] = newNode\n",
    "                            parentn = newNode\n",
    "                        elif len(parentn.childD)+1 == self.maxChild:\n",
    "                            newNode = Node(depth=currentDepth+1, digitOrtoken='<*>')\n",
    "                            parentn.childD['<*>'] = newNode\n",
    "                            parentn = newNode\n",
    "                        else:\n",
    "                            parentn = parentn.childD['<*>']\n",
    "            \n",
    "                else:\n",
    "                    if '<*>' not in parentn.childD:\n",
    "                        newNode = Node(depth=currentDepth+1, digitOrtoken='<*>')\n",
    "                        parentn.childD['<*>'] = newNode\n",
    "                        parentn = newNode\n",
    "                    else:\n",
    "                        parentn = parentn.childD['<*>']\n",
    "\n",
    "            #If the token is matched\n",
    "            else:\n",
    "                parentn = parentn.childD[token]\n",
    "\n",
    "            currentDepth += 1\n",
    "\n",
    "    #seq1 is template\n",
    "    def seqDist(self, seq1, seq2):\n",
    "        assert len(seq1) == len(seq2)\n",
    "        simTokens = 0\n",
    "        numOfPar = 0\n",
    "\n",
    "        for token1, token2 in zip(seq1, seq2):\n",
    "            if token1 == '<*>':\n",
    "                numOfPar += 1\n",
    "                continue\n",
    "            if token1 == token2:\n",
    "                simTokens += 1 \n",
    "\n",
    "        retVal = float(simTokens) / len(seq1)\n",
    "\n",
    "        return retVal, numOfPar\n",
    "\n",
    "\n",
    "    def fastMatch(self, logClustL, seq):\n",
    "        retLogClust = None\n",
    "\n",
    "        maxSim = -1\n",
    "        maxNumOfPara = -1\n",
    "        maxClust = None\n",
    "\n",
    "        for logClust in logClustL:\n",
    "            curSim, curNumOfPara = self.seqDist(logClust.logTemplate, seq)\n",
    "            if curSim>maxSim or (curSim==maxSim and curNumOfPara>maxNumOfPara):\n",
    "                maxSim = curSim\n",
    "                maxNumOfPara = curNumOfPara\n",
    "                maxClust = logClust\n",
    "\n",
    "        if maxSim >= self.st:\n",
    "            retLogClust = maxClust  \n",
    "\n",
    "        return retLogClust\n",
    "\n",
    "    def getTemplate(self, seq1, seq2):\n",
    "        assert len(seq1) == len(seq2)\n",
    "        retVal = []\n",
    "\n",
    "        i = 0\n",
    "        for word in seq1:\n",
    "            if word == seq2[i]:\n",
    "                retVal.append(word)\n",
    "            else:\n",
    "                retVal.append('<*>')\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        return retVal\n",
    "\n",
    "    def outputResult(self, logClustL):\n",
    "        log_templates = [0] * self.df_log.shape[0]\n",
    "        log_templateids = [0] * self.df_log.shape[0]\n",
    "        df_events = []\n",
    "        for logClust in logClustL:\n",
    "            template_str = ' '.join(logClust.logTemplate)\n",
    "            occurrence = len(logClust.logIDL)\n",
    "            template_id = hashlib.md5(template_str.encode('utf-8')).hexdigest()[0:8]\n",
    "            for logID in logClust.logIDL:\n",
    "                logID -= 1\n",
    "                log_templates[logID] = template_str\n",
    "                log_templateids[logID] = template_id\n",
    "            df_events.append([template_id, template_str, occurrence])\n",
    "\n",
    "        df_event = pd.DataFrame(df_events, columns=['EventId', 'EventTemplate', 'Occurrences'])\n",
    "        self.df_log['EventId'] = log_templateids\n",
    "        self.df_log['EventTemplate'] = log_templates\n",
    "        if self.keep_para:\n",
    "            self.df_log[\"ParameterList\"] = self.df_log.apply(self.get_parameter_list, axis=1) \n",
    "        self.df_log.to_csv(os.path.join(self.savePath, self.logName + '_structured.csv'), index=False)\n",
    "\n",
    "\n",
    "        occ_dict = dict(self.df_log['EventTemplate'].value_counts())\n",
    "        df_event = pd.DataFrame()\n",
    "        df_event['EventTemplate'] = self.df_log['EventTemplate'].unique()\n",
    "        df_event['EventId'] = df_event['EventTemplate'].map(lambda x: hashlib.md5(x.encode('utf-8')).hexdigest()[0:8])\n",
    "        df_event['Occurrences'] = df_event['EventTemplate'].map(occ_dict)\n",
    "        df_event.to_csv(os.path.join(self.savePath, self.logName + '_templates.csv'), index=False, columns=[\"EventId\", \"EventTemplate\", \"Occurrences\"])\n",
    "\n",
    "\n",
    "    def printTree(self, node, dep):\n",
    "        pStr = ''   \n",
    "        for i in range(dep):\n",
    "            pStr += '\\t'\n",
    "\n",
    "        if node.depth == 0:\n",
    "            pStr += 'Root'\n",
    "        elif node.depth == 1:\n",
    "            pStr += '<' + str(node.digitOrtoken) + '>'\n",
    "        else:\n",
    "            pStr += node.digitOrtoken\n",
    "\n",
    "        print(pStr)\n",
    "\n",
    "        if node.depth == self.depth:\n",
    "            return 1\n",
    "        for child in node.childD:\n",
    "            self.printTree(node.childD[child], dep+1)\n",
    "\n",
    "\n",
    "    def parse(self, logName):\n",
    "        print('Parsing file: ' + os.path.join(self.path, logName))\n",
    "        start_time = datetime.now()\n",
    "        self.logName = logName\n",
    "        rootNode = Node()\n",
    "        logCluL = []\n",
    "\n",
    "        self.load_data()\n",
    "\n",
    "        count = 0\n",
    "        for idx, line in self.df_log.iterrows():\n",
    "            logID = line['LineId']\n",
    "            logmessageL = self.preprocess(line['Content']).strip().split()\n",
    "            # logmessageL = filter(lambda x: x != '', re.split('[\\s=:,]', self.preprocess(line['Content'])))\n",
    "            matchCluster = self.treeSearch(rootNode, logmessageL)\n",
    "\n",
    "            #Match no existing log cluster\n",
    "            if matchCluster is None:\n",
    "                newCluster = Logcluster(logTemplate=logmessageL, logIDL=[logID])\n",
    "                logCluL.append(newCluster)\n",
    "                self.addSeqToPrefixTree(rootNode, newCluster)\n",
    "\n",
    "            #Add the new log message to the existing cluster\n",
    "            else:\n",
    "                newTemplate = self.getTemplate(logmessageL, matchCluster.logTemplate)\n",
    "                matchCluster.logIDL.append(logID)\n",
    "                if ' '.join(newTemplate) != ' '.join(matchCluster.logTemplate): \n",
    "                    matchCluster.logTemplate = newTemplate\n",
    "\n",
    "            count += 1\n",
    "            if count % 1000 == 0 or count == len(self.df_log):\n",
    "                print('Processed {0:.1f}% of log lines.'.format(count * 100.0 / len(self.df_log)))\n",
    "\n",
    "\n",
    "        if not os.path.exists(self.savePath):\n",
    "            os.makedirs(self.savePath)\n",
    "\n",
    "        self.outputResult(logCluL)\n",
    "\n",
    "        print('Parsing done. [Time taken: {!s}]'.format(datetime.now() - start_time))\n",
    "\n",
    "    def load_data(self):\n",
    "        headers, regex = self.generate_logformat_regex(self.log_format)\n",
    "        self.df_log = self.log_to_dataframe(os.path.join(self.path, self.logName), regex, headers, self.log_format)\n",
    "\n",
    "    def preprocess(self, line):\n",
    "        for currentRex in self.rex:\n",
    "            line = re.sub(currentRex, '<*>', line)\n",
    "        return line\n",
    "\n",
    "    def log_to_dataframe(self, log_file, regex, headers, logformat):\n",
    "        \"\"\" Function to transform log file to dataframe \n",
    "        \"\"\"\n",
    "        log_messages = []\n",
    "        linecount = 0\n",
    "        with open(log_file, 'r') as fin:\n",
    "            for line in fin.readlines():\n",
    "                try:\n",
    "                    match = regex.search(line.strip())\n",
    "                    message = [match.group(header) for header in headers]\n",
    "                    log_messages.append(message)\n",
    "                    linecount += 1\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "        logdf = pd.DataFrame(log_messages, columns=headers)\n",
    "        logdf.insert(0, 'LineId', None)\n",
    "        logdf['LineId'] = [i + 1 for i in range(linecount)]\n",
    "        return logdf\n",
    "\n",
    "\n",
    "    def generate_logformat_regex(self, logformat):\n",
    "        \"\"\" Function to generate regular expression to split log messages\n",
    "        \"\"\"\n",
    "        headers = []\n",
    "        splitters = re.split(r'(<[^<>]+>)', logformat)\n",
    "        regex = ''\n",
    "        for k in range(len(splitters)):\n",
    "            if k % 2 == 0:\n",
    "                splitter = re.sub(' +', '\\\\\\s+', splitters[k])\n",
    "                regex += splitter\n",
    "            else:\n",
    "                header = splitters[k].strip('<').strip('>')\n",
    "                regex += '(?P<%s>.*?)' % header\n",
    "                headers.append(header)\n",
    "        regex = re.compile('^' + regex + '$')\n",
    "        return headers, regex\n",
    "\n",
    "    def get_parameter_list(self, row):\n",
    "        template_regex = re.sub(r\"<.{1,5}>\", \"<*>\", row[\"EventTemplate\"])\n",
    "        if \"<*>\" not in template_regex: return []\n",
    "        template_regex = re.sub(r'([^A-Za-z0-9])', r'\\\\\\1', template_regex)\n",
    "#         template_regex = re.sub(r'\\\\ +', r'\\s+', template_regex)\n",
    "        template_regex = \"^\" + template_regex.replace(\"\\<\\*\\>\", \"(.*?)\") + \"$\"\n",
    "        parameter_list = re.findall(template_regex, row[\"Content\"])\n",
    "        parameter_list = parameter_list[0] if parameter_list else ()\n",
    "        parameter_list = list(parameter_list) if isinstance(parameter_list, tuple) else [parameter_list]\n",
    "        return parameter_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "024d7e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing file: HDFS_2k.log\n",
      "Processed 50.0% of log lines.\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.197455]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# from logparser import Drain\n",
    "\n",
    "input_dir = \"\"  # The input directory of log file\n",
    "output_dir = \"example_parsed/\"  # The output directory of parsing results\n",
    "log_file = \"HDFS_2k.log\"  # The input log file name\n",
    "log_format = \"<Date> <Time> <Pid> <Level> <Component>: <Content>\"  # HDFS log format\n",
    "# Regular expression list for optional preprocessing (default: [])\n",
    "regex = [\n",
    "    r\"blk_(|-)[0-9]+\",  # block id\n",
    "    r\"(/|)([0-9]+\\.){3}[0-9]+(:[0-9]+|)(:|)\",  # IP\n",
    "    r\"(?<=[^A-Za-z0-9])(\\-?\\+?\\d+)(?=[^A-Za-z0-9])|[0-9]+$\",  # Numbers\n",
    "]\n",
    "st = 0.5  # Similarity threshold\n",
    "depth = 4  # Depth of all leaf nodes\n",
    "\n",
    "parser = LogParser(\n",
    "    log_format, indir=input_dir, outdir=output_dir, depth=depth, st=st, rex=regex\n",
    ")\n",
    "parser.parse(log_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de97938",
   "metadata": {},
   "source": [
    "## data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2055e4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The interface to load log datasets. The datasets currently supported include\n",
    "HDFS and BGL.\n",
    "\n",
    "Authors:\n",
    "    LogPAI Team\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "from collections import OrderedDict\n",
    "\n",
    "def _split_data(x_data, y_data=None, train_ratio=0, split_type='uniform'):\n",
    "    if split_type == 'uniform' and y_data is not None:\n",
    "        pos_idx = y_data > 0\n",
    "        x_pos = x_data[pos_idx]\n",
    "        y_pos = y_data[pos_idx]\n",
    "        x_neg = x_data[~pos_idx]\n",
    "        y_neg = y_data[~pos_idx]\n",
    "        train_pos = int(train_ratio * x_pos.shape[0])\n",
    "        train_neg = int(train_ratio * x_neg.shape[0])\n",
    "        x_train = np.hstack([x_pos[0:train_pos], x_neg[0:train_neg]])\n",
    "        y_train = np.hstack([y_pos[0:train_pos], y_neg[0:train_neg]])\n",
    "        x_test = np.hstack([x_pos[train_pos:], x_neg[train_neg:]])\n",
    "        y_test = np.hstack([y_pos[train_pos:], y_neg[train_neg:]])\n",
    "    elif split_type == 'sequential':\n",
    "        num_train = int(train_ratio * x_data.shape[0])\n",
    "        x_train = x_data[0:num_train]\n",
    "        x_test = x_data[num_train:]\n",
    "        if y_data is None:\n",
    "            y_train = None\n",
    "            y_test = None\n",
    "        else:\n",
    "            y_train = y_data[0:num_train]\n",
    "            y_test = y_data[num_train:]\n",
    "    # Random shuffle\n",
    "    indexes = shuffle(np.arange(x_train.shape[0]))\n",
    "    x_train = x_train[indexes]\n",
    "    if y_train is not None:\n",
    "        y_train = y_train[indexes]\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "def load_HDFS(log_file, label_file=None, window='session', train_ratio=0.5, split_type='sequential', save_csv=False, window_size=0):\n",
    "    \"\"\" Load HDFS structured log into train and test data\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "        log_file: str, the file path of structured log.\n",
    "        label_file: str, the file path of anomaly labels, None for unlabeled data\n",
    "        window: str, the window options including `session` (default).\n",
    "        train_ratio: float, the ratio of training data for train/test split.\n",
    "        split_type: `uniform` or `sequential`, which determines how to split dataset. `uniform` means\n",
    "            to split positive samples and negative samples equally when setting label_file. `sequential`\n",
    "            means to split the data sequentially without label_file. That is, the first part is for training,\n",
    "            while the second part is for testing.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        (x_train, y_train): the training data\n",
    "        (x_test, y_test): the testing data\n",
    "    \"\"\"\n",
    "\n",
    "    print('====== Input data summary ======')\n",
    "\n",
    "    if log_file.endswith('.npz'):\n",
    "        # Split training and validation set in a class-uniform way\n",
    "        data = np.load(log_file)\n",
    "        x_data = data['x_data']\n",
    "        y_data = data['y_data']\n",
    "        (x_train, y_train), (x_test, y_test) = _split_data(x_data, y_data, train_ratio, split_type)\n",
    "\n",
    "    elif log_file.endswith('.csv'):\n",
    "        assert window == 'session', \"Only window=session is supported for HDFS dataset.\"\n",
    "        print(\"Loading\", log_file)\n",
    "        struct_log = pd.read_csv(log_file, engine='c',\n",
    "                na_filter=False, memory_map=True)\n",
    "        data_dict = OrderedDict()\n",
    "        for idx, row in struct_log.iterrows():\n",
    "            blkId_list = re.findall(r'(blk_-?\\d+)', row['Content'])\n",
    "            blkId_set = set(blkId_list)\n",
    "            for blk_Id in blkId_set:\n",
    "                if not blk_Id in data_dict:\n",
    "                    data_dict[blk_Id] = []\n",
    "                data_dict[blk_Id].append(row['EventId'])\n",
    "        data_df = pd.DataFrame(list(data_dict.items()), columns=['BlockId', 'EventSequence'])\n",
    "        \n",
    "        if label_file:\n",
    "            # Split training and validation set in a class-uniform way\n",
    "            label_data = pd.read_csv(label_file, engine='c', na_filter=False, memory_map=True)\n",
    "            label_data = label_data.set_index('BlockId')\n",
    "            label_dict = label_data['Label'].to_dict()\n",
    "            data_df['Label'] = data_df['BlockId'].apply(lambda x: 1 if label_dict[x] == 'Anomaly' else 0)\n",
    "\n",
    "            # Split train and test data\n",
    "            (x_train, y_train), (x_test, y_test) = _split_data(data_df['EventSequence'].values, \n",
    "                data_df['Label'].values, train_ratio, split_type)\n",
    "            print(data_df['EventSequence'].values,data_df['Label'].values)\n",
    "            print(y_train.sum(), y_test.sum())\n",
    "\n",
    "        if save_csv:\n",
    "            data_df.to_csv('data_instances.csv', index=False)\n",
    "\n",
    "        if window_size > 0:\n",
    "            x_train, window_y_train, y_train = slice_hdfs(x_train, y_train, window_size)\n",
    "            x_test, window_y_test, y_test = slice_hdfs(x_test, y_test, window_size)\n",
    "            log = \"{} {} windows ({}/{} anomaly), {}/{} normal\"\n",
    "            print(log.format(\"Train:\", x_train.shape[0], y_train.sum(), y_train.shape[0], (1-y_train).sum(), y_train.shape[0]))\n",
    "            print(log.format(\"Test:\", x_test.shape[0], y_test.sum(), y_test.shape[0], (1-y_test).sum(), y_test.shape[0]))\n",
    "            return (x_train, window_y_train, y_train), (x_test, window_y_test, y_test)\n",
    "\n",
    "        if label_file is None:\n",
    "            if split_type == 'uniform':\n",
    "                split_type = 'sequential'\n",
    "                print('Warning: Only split_type=sequential is supported \\\n",
    "                if label_file=None.'.format(split_type))\n",
    "            # Split training and validation set sequentially\n",
    "            x_data = data_df['EventSequence'].values\n",
    "            (x_train, _), (x_test, _) = _split_data(x_data, train_ratio=train_ratio, split_type=split_type)\n",
    "            print('Total: {} instances, train: {} instances, test: {} instances'.format(\n",
    "                  x_data.shape[0], x_train.shape[0], x_test.shape[0]))\n",
    "            return (x_train, None), (x_test, None), data_df\n",
    "    else:\n",
    "        raise NotImplementedError('load_HDFS() only support csv and npz files!')\n",
    "\n",
    "    num_train = x_train.shape[0]\n",
    "    num_test = x_test.shape[0]\n",
    "    num_total = num_train + num_test\n",
    "    num_train_pos = sum(y_train)\n",
    "    num_test_pos = sum(y_test)\n",
    "    num_pos = num_train_pos + num_test_pos\n",
    "\n",
    "    print('Total: {} instances, {} anomaly, {} normal' \\\n",
    "          .format(num_total, num_pos, num_total - num_pos))\n",
    "    print('Train: {} instances, {} anomaly, {} normal' \\\n",
    "          .format(num_train, num_train_pos, num_train - num_train_pos))\n",
    "    print('Test: {} instances, {} anomaly, {} normal\\n' \\\n",
    "          .format(num_test, num_test_pos, num_test - num_test_pos))\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "def slice_hdfs(x, y, window_size):\n",
    "    results_data = []\n",
    "    print(\"Slicing {} sessions, with window {}\".format(x.shape[0], window_size))\n",
    "    for idx, sequence in enumerate(x):\n",
    "        seqlen = len(sequence)\n",
    "        i = 0\n",
    "        while (i + window_size) < seqlen:\n",
    "            slice = sequence[i: i + window_size]\n",
    "            results_data.append([idx, slice, sequence[i + window_size], y[idx]])\n",
    "            i += 1\n",
    "        else:\n",
    "            slice = sequence[i: i + window_size]\n",
    "            slice += [\"#Pad\"] * (window_size - len(slice))\n",
    "            results_data.append([idx, slice, \"#Pad\", y[idx]])\n",
    "    results_df = pd.DataFrame(results_data, columns=[\"SessionId\", \"EventSequence\", \"Label\", \"SessionLabel\"])\n",
    "    print(\"Slicing done, {} windows generated\".format(results_df.shape[0]))\n",
    "    return results_df[[\"SessionId\", \"EventSequence\"]], results_df[\"Label\"], results_df[\"SessionLabel\"]\n",
    "\n",
    "\n",
    "\n",
    "def load_BGL(log_file, label_file=None, window='sliding', time_interval=60, stepping_size=60, \n",
    "             train_ratio=0.8):\n",
    "    \"\"\"  TODO\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def bgl_preprocess_data(para, raw_data, event_mapping_data):\n",
    "    \"\"\" split logs into sliding windows, built an event count matrix and get the corresponding label\n",
    "\n",
    "    Args:\n",
    "    --------\n",
    "    para: the parameters dictionary\n",
    "    raw_data: list of (label, time)\n",
    "    event_mapping_data: a list of event index, where each row index indicates a corresponding log\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    event_count_matrix: event count matrix, where each row is an instance (log sequence vector)\n",
    "    labels: a list of labels, 1 represents anomaly\n",
    "    \"\"\"\n",
    "\n",
    "    # create the directory for saving the sliding windows (start_index, end_index), which can be directly loaded in future running\n",
    "    if not os.path.exists(para['save_path']):\n",
    "        os.mkdir(para['save_path'])\n",
    "    log_size = raw_data.shape[0]\n",
    "    sliding_file_path = para['save_path']+'sliding_'+str(para['window_size'])+'h_'+str(para['step_size'])+'h.csv'\n",
    "\n",
    "    #=============divide into sliding windows=========#\n",
    "    start_end_index_list = [] # list of tuples, tuple contains two number, which represent the start and end of sliding time window\n",
    "    label_data, time_data = raw_data[:,0], raw_data[:, 1]\n",
    "    if not os.path.exists(sliding_file_path):\n",
    "        # split into sliding window\n",
    "        start_time = time_data[0]\n",
    "        start_index = 0\n",
    "        end_index = 0\n",
    "\n",
    "        # get the first start, end index, end time\n",
    "        for cur_time in time_data:\n",
    "            if  cur_time < start_time + para['window_size']*3600:\n",
    "                end_index += 1\n",
    "                end_time = cur_time\n",
    "            else:\n",
    "                start_end_pair=tuple((start_index,end_index))\n",
    "                start_end_index_list.append(start_end_pair)\n",
    "                break\n",
    "        # move the start and end index until next sliding window\n",
    "        while end_index < log_size:\n",
    "            start_time = start_time + para['step_size']*3600\n",
    "            end_time = end_time + para['step_size']*3600\n",
    "            for i in range(start_index,end_index):\n",
    "                if time_data[i] < start_time:\n",
    "                    i+=1\n",
    "                else:\n",
    "                    break\n",
    "            for j in range(end_index, log_size):\n",
    "                if time_data[j] < end_time:\n",
    "                    j+=1\n",
    "                else:\n",
    "                    break\n",
    "            start_index = i\n",
    "            end_index = j\n",
    "            start_end_pair = tuple((start_index, end_index))\n",
    "            start_end_index_list.append(start_end_pair)\n",
    "        inst_number = len(start_end_index_list)\n",
    "        print('there are %d instances (sliding windows) in this dataset\\n'%inst_number)\n",
    "        np.savetxt(sliding_file_path,start_end_index_list,delimiter=',',fmt='%d')\n",
    "    else:\n",
    "        print('Loading start_end_index_list from file')\n",
    "        start_end_index_list = pd.read_csv(sliding_file_path, header=None).values\n",
    "        inst_number = len(start_end_index_list)\n",
    "        print('there are %d instances (sliding windows) in this dataset' % inst_number)\n",
    "\n",
    "    # get all the log indexes in each time window by ranging from start_index to end_index\n",
    "    expanded_indexes_list=[]\n",
    "    for t in range(inst_number):\n",
    "        index_list = []\n",
    "        expanded_indexes_list.append(index_list)\n",
    "    for i in range(inst_number):\n",
    "        start_index = start_end_index_list[i][0]\n",
    "        end_index = start_end_index_list[i][1]\n",
    "        for l in range(start_index, end_index):\n",
    "            expanded_indexes_list[i].append(l)\n",
    "\n",
    "    event_mapping_data = [row[0] for row in event_mapping_data]\n",
    "    event_num = len(list(set(event_mapping_data)))\n",
    "    print('There are %d log events'%event_num)\n",
    "\n",
    "    #=============get labels and event count of each sliding window =========#\n",
    "    labels = []\n",
    "    event_count_matrix = np.zeros((inst_number,event_num))\n",
    "    for j in range(inst_number):\n",
    "        label = 0   #0 represent success, 1 represent failure\n",
    "        for k in expanded_indexes_list[j]:\n",
    "            event_index = event_mapping_data[k]\n",
    "            event_count_matrix[j, event_index] += 1\n",
    "            if label_data[k]:\n",
    "                label = 1\n",
    "                continue\n",
    "        labels.append(label)\n",
    "    assert inst_number == len(labels)\n",
    "    print(\"Among all instances, %d are anomalies\"%sum(labels))\n",
    "    assert event_count_matrix.shape[0] == len(labels)\n",
    "    return event_count_matrix, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd95e489",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c835ea71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The interface for data preprocessing.\n",
    "\n",
    "Authors:\n",
    "    LogPAI Team\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from scipy.special import expit\n",
    "from itertools import compress\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class Iterator(Dataset):\n",
    "    def __init__(self, data_dict, batch_size=32, shuffle=False, num_workers=1):\n",
    "        self.data_dict = data_dict\n",
    "        self.keys = list(data_dict.keys())\n",
    "        self.iter = DataLoader(dataset=self, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {k: self.data_dict[k][index] for k in self.keys}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_dict[\"SessionId\"].shape[0]\n",
    "\n",
    "class Vectorizer(object):\n",
    "\n",
    "    def fit_transform(self, x_train, window_y_train, y_train):\n",
    "        self.label_mapping = {eid: idx for idx, eid in enumerate(window_y_train.unique(), 2)}\n",
    "        self.label_mapping[\"#OOV\"] = 0\n",
    "        self.label_mapping[\"#Pad\"] = 1\n",
    "        self.num_labels = len(self.label_mapping)\n",
    "        return self.transform(x_train, window_y_train, y_train)\n",
    "\n",
    "    def transform(self, x, window_y, y):\n",
    "        x[\"EventSequence\"] = x[\"EventSequence\"].map(lambda x: [self.label_mapping.get(item, 0) for item in x])\n",
    "        window_y = window_y.map(lambda x: self.label_mapping.get(x, 0))\n",
    "        y = y\n",
    "        data_dict = {\"SessionId\": x[\"SessionId\"].values, \"window_y\": window_y.values, \"y\": y.values, \"x\": np.array(x[\"EventSequence\"].tolist())}\n",
    "        return data_dict\n",
    "        \n",
    "\n",
    "class FeatureExtractor(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.idf_vec = None\n",
    "        self.mean_vec = None\n",
    "        self.events = None\n",
    "        self.term_weighting = None\n",
    "        self.normalization = None\n",
    "        self.oov = None\n",
    "\n",
    "    def fit_transform(self, X_seq, term_weighting=None, normalization=None, oov=False, min_count=1):\n",
    "        \"\"\" Fit and transform the data matrix\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "            X_seq: ndarray, log sequences matrix\n",
    "            term_weighting: None or `tf-idf`\n",
    "            normalization: None or `zero-mean`\n",
    "            oov: bool, whether to use OOV event\n",
    "            min_count: int, the minimal occurrence of events (default 0), only valid when oov=True.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            X_new: The transformed data matrix\n",
    "        \"\"\"\n",
    "        print('====== Transformed train data summary ======')\n",
    "        self.term_weighting = term_weighting\n",
    "        self.normalization = normalization\n",
    "        self.oov = oov\n",
    "\n",
    "        X_counts = []\n",
    "        for i in range(X_seq.shape[0]):\n",
    "            event_counts = Counter(X_seq[i])\n",
    "            X_counts.append(event_counts)\n",
    "        X_df = pd.DataFrame(X_counts)\n",
    "        X_df = X_df.fillna(0)\n",
    "        self.events = X_df.columns\n",
    "        X = X_df.values\n",
    "        if self.oov:\n",
    "            oov_vec = np.zeros(X.shape[0])\n",
    "            if min_count > 1:\n",
    "                idx = np.sum(X > 0, axis=0) >= min_count\n",
    "                oov_vec = np.sum(X[:, ~idx] > 0, axis=1)\n",
    "                X = X[:, idx]\n",
    "                self.events = np.array(X_df.columns)[idx].tolist()\n",
    "            X = np.hstack([X, oov_vec.reshape(X.shape[0], 1)])\n",
    "        \n",
    "        num_instance, num_event = X.shape\n",
    "        if self.term_weighting == 'tf-idf':\n",
    "            df_vec = np.sum(X > 0, axis=0)\n",
    "            self.idf_vec = np.log(num_instance / (df_vec + 1e-8))\n",
    "            idf_matrix = X * np.tile(self.idf_vec, (num_instance, 1)) \n",
    "            X = idf_matrix\n",
    "        if self.normalization == 'zero-mean':\n",
    "            mean_vec = X.mean(axis=0)\n",
    "            self.mean_vec = mean_vec.reshape(1, num_event)\n",
    "            X = X - np.tile(self.mean_vec, (num_instance, 1))\n",
    "        elif self.normalization == 'sigmoid':\n",
    "            X[X != 0] = expit(X[X != 0])\n",
    "        X_new = X\n",
    "        \n",
    "        print('Train data shape: {}-by-{}\\n'.format(X_new.shape[0], X_new.shape[1])) \n",
    "        return X_new\n",
    "\n",
    "    def transform(self, X_seq):\n",
    "        \"\"\" Transform the data matrix with trained parameters\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "            X: log sequences matrix\n",
    "            term_weighting: None or `tf-idf`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            X_new: The transformed data matrix\n",
    "        \"\"\"\n",
    "        print('====== Transformed test data summary ======')\n",
    "        X_counts = []\n",
    "        for i in range(X_seq.shape[0]):\n",
    "            event_counts = Counter(X_seq[i])\n",
    "            X_counts.append(event_counts)\n",
    "        X_df = pd.DataFrame(X_counts)\n",
    "        X_df = X_df.fillna(0)\n",
    "        empty_events = set(self.events) - set(X_df.columns)\n",
    "        for event in empty_events:\n",
    "            X_df[event] = [0] * len(X_df)\n",
    "        X = X_df[self.events].values\n",
    "        if self.oov:\n",
    "            oov_vec = np.sum(X_df[X_df.columns.difference(self.events)].values > 0, axis=1)\n",
    "            X = np.hstack([X, oov_vec.reshape(X.shape[0], 1)])\n",
    "        \n",
    "        num_instance, num_event = X.shape\n",
    "        if self.term_weighting == 'tf-idf':\n",
    "            idf_matrix = X * np.tile(self.idf_vec, (num_instance, 1)) \n",
    "            X = idf_matrix\n",
    "        if self.normalization == 'zero-mean':\n",
    "            X = X - np.tile(self.mean_vec, (num_instance, 1))\n",
    "        elif self.normalization == 'sigmoid':\n",
    "            X[X != 0] = expit(X[X != 0])\n",
    "        X_new = X\n",
    "\n",
    "        print('Test data shape: {}-by-{}\\n'.format(X_new.shape[0], X_new.shape[1])) \n",
    "\n",
    "        return X_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aa23450",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def metrics(y_pred, y_true):\n",
    "    \"\"\" Calucate evaluation metrics for precision, recall, and f1.\n",
    "    Arguments\n",
    "    ---------\n",
    "        y_pred: ndarry, the predicted result list\n",
    "        y_true: ndarray, the ground truth label list\n",
    "    Returns\n",
    "    -------\n",
    "        precision: float, precision value\n",
    "        recall: float, recall value\n",
    "        f1: float, f1 measure value\n",
    "    \"\"\"\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    return precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9e5a9a",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbf235b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "The implementation of Log Clustering model for anomaly detection.\n",
    "Authors: \n",
    "    LogPAI Team\n",
    "Reference: \n",
    "    [1] Qingwei Lin, Hongyu Zhang, Jian-Guang Lou, Yu Zhang, Xuewei Chen. Log Clustering \n",
    "        based Problem Identification for Online Service Systems. International Conference\n",
    "        on Software Engineering (ICSE), 2016.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pprint\n",
    "from scipy.special import expit\n",
    "from numpy import linalg as LA\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "\n",
    "class LogClustering(object):\n",
    "\n",
    "    def __init__(self, max_dist=0.3, anomaly_threshold=0.3, mode='online', num_bootstrap_samples=1000):\n",
    "        \"\"\"\n",
    "        Attributes\n",
    "        ----------\n",
    "            max_dist: float, the threshold to stop the clustering process\n",
    "            anomaly_threshold: float, the threshold for anomaly detection\n",
    "            mode: str, 'offline' or 'online' mode for clustering\n",
    "            num_bootstrap_samples: int, online clustering starts with a bootstraping process, which\n",
    "                determines the initial cluster representatives offline using a subset of samples \n",
    "            representatives: ndarray, the representative samples of clusters, of shape \n",
    "                num_clusters-by-num_events\n",
    "            cluster_size_dict: dict, the size of each cluster, used to update representatives online \n",
    "        \"\"\"\n",
    "        self.max_dist = 0.3\n",
    "        self.anomaly_threshold = anomaly_threshold\n",
    "        self.mode = mode\n",
    "        self.num_bootstrap_samples = num_bootstrap_samples\n",
    "        self.representatives = list()\n",
    "        self.cluster_size_dict = dict()\n",
    "\n",
    "    def fit(self, X):   \n",
    "        print('====== Model summary ======')         \n",
    "        if self.mode == 'offline':\n",
    "            # The offline mode can process about 10K samples only due to huge memory consumption.\n",
    "            self._offline_clustering(X)\n",
    "        elif self.mode == 'online':\n",
    "            # Bootstrapping phase\n",
    "            if self.num_bootstrap_samples > 0:\n",
    "                X_bootstrap = X[0:self.num_bootstrap_samples, :]\n",
    "                self._offline_clustering(X_bootstrap)\n",
    "            # Online learning phase\n",
    "            if X.shape[0] > self.num_bootstrap_samples:\n",
    "                self._online_clustering(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.zeros(X.shape[0])\n",
    "        for i in range(X.shape[0]):\n",
    "            min_dist, min_index = self._get_min_cluster_dist(X[i, :])\n",
    "            if min_dist > self.anomaly_threshold:\n",
    "                y_pred[i] = 1\n",
    "        return y_pred\n",
    "\n",
    "    def evaluate(self, X, y_true):\n",
    "        print('====== Evaluation summary ======')\n",
    "        y_pred = self.predict(X)\n",
    "        precision, recall, f1 = metrics(y_pred, y_true)\n",
    "        print('Precision: {:.3f}, recall: {:.3f}, F1-measure: {:.3f}\\n' \\\n",
    "              .format(precision, recall, f1))\n",
    "        return precision, recall, f1\n",
    "\n",
    "    def _offline_clustering(self, X):\n",
    "        print('Starting offline clustering...')\n",
    "        p_dist = pdist(X, metric=self._distance_metric)\n",
    "        Z = linkage(p_dist, 'complete')\n",
    "        cluster_index = fcluster(Z, self.max_dist, criterion='distance')\n",
    "        self._extract_representatives(X, cluster_index)\n",
    "        print('Processed {} instances.'.format(X.shape[0]))\n",
    "        print('Found {} clusters offline.\\n'.format(len(self.representatives)))\n",
    "        # print('The representive vectors are:')\n",
    "        # pprint.pprint(self.representatives.tolist())\n",
    "\n",
    "    def _extract_representatives(self, X, cluster_index):\n",
    "        num_clusters = len(set(cluster_index))\n",
    "        for clu in range(num_clusters):\n",
    "            clu_idx = np.argwhere(cluster_index == clu + 1)[:, 0]\n",
    "            self.cluster_size_dict[clu] = clu_idx.shape[0]\n",
    "            repre_center = np.average(X[clu_idx, :], axis=0)\n",
    "            self.representatives.append(repre_center)\n",
    "\n",
    "    def _online_clustering(self, X):\n",
    "        print(\"Starting online clustering...\")\n",
    "        for i in range(self.num_bootstrap_samples, X.shape[0]):\n",
    "            if (i + 1) % 2000 == 0:\n",
    "                print('Processed {} instances.'.format(i + 1))\n",
    "            instance_vec = X[i, :]\n",
    "            if len(self.representatives) > 0:\n",
    "                min_dist, clu_id = self._get_min_cluster_dist(instance_vec)\n",
    "                if min_dist <= self.max_dist:\n",
    "                    self.cluster_size_dict[clu_id] += 1\n",
    "                    self.representatives[clu_id] = self.representatives[clu_id] \\\n",
    "                                                 + (instance_vec - self.representatives[clu_id]) \\\n",
    "                                                 / self.cluster_size_dict[clu_id]\n",
    "                    continue\n",
    "            self.cluster_size_dict[len(self.representatives)] = 1\n",
    "            self.representatives.append(instance_vec)\n",
    "        print('Processed {} instances.'.format(X.shape[0]))\n",
    "        print('Found {} clusters online.\\n'.format(len(self.representatives)))\n",
    "        # print('The representive vectors are:')\n",
    "        # pprint.pprint(self.representatives.tolist())\n",
    "\n",
    "    def _distance_metric(self, x1, x2):\n",
    "        norm= LA.norm(x1) * LA.norm(x2)\n",
    "        distance = 1 - np.dot(x1, x2) / (norm + 1e-8)\n",
    "        if distance < 1e-8:\n",
    "            distance = 0\n",
    "        return distance\n",
    "\n",
    "    def _get_min_cluster_dist(self, instance_vec):\n",
    "        min_index = -1\n",
    "        min_dist = float('inf')\n",
    "        for i in range(len(self.representatives)):\n",
    "            cluster_rep = self.representatives[i]\n",
    "            dist = self._distance_metric(instance_vec, cluster_rep)\n",
    "            if dist < 1e-8:\n",
    "                min_dist = 0\n",
    "                min_index = i\n",
    "                break\n",
    "            elif dist < min_dist:\n",
    "                min_dist = dist\n",
    "                min_index = i\n",
    "        return min_dist, min_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9193b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest as iForest\n",
    "\n",
    "class IsolationForest(iForest):\n",
    "\n",
    "    def __init__(self, n_estimators=100, max_samples='auto', contamination=0.03, **kwargs):\n",
    "        \"\"\" The IsolationForest model for anomaly detection\n",
    "        Arguments\n",
    "        ---------\n",
    "            n_estimators : int, optional (default=100). The number of base estimators in the ensemble.\n",
    "            max_samples : int or float, optional (default=\"auto\")\n",
    "                The number of samples to draw from X to train each base estimator.\n",
    "                    - If int, then draw max_samples samples.\n",
    "                    - If float, then draw max_samples * X.shape[0] samples.\n",
    "                    - If \"auto\", then max_samples=min(256, n_samples).\n",
    "                If max_samples is larger than the number of samples provided, all samples will be used \n",
    "                for all trees (no sampling).\n",
    "            contamination : float in (0., 0.5), optional (default='auto')\n",
    "                The amount of contamination of the data set, i.e. the proportion of outliers in the data \n",
    "                set. Used when fitting to define the threshold on the decision function. If 'auto', the \n",
    "                decision function threshold is determined as in the original paper.\n",
    "            max_features : int or float, optional (default=1.0)\n",
    "                The number of features to draw from X to train each base estimator.\n",
    "                    - If int, then draw max_features features.\n",
    "                    - If float, then draw max_features * X.shape[1] features.\n",
    "            bootstrap : boolean, optional (default=False)\n",
    "                If True, individual trees are fit on random subsets of the training data sampled with replacement. \n",
    "                If False, sampling without replacement is performed.\n",
    "            n_jobs : int or None, optional (default=None)\n",
    "                The number of jobs to run in parallel for both fit and predict. None means 1 unless in a \n",
    "                joblib.parallel_backend context. -1 means using all processors. \n",
    "            random_state : int, RandomState instance or None, optional (default=None)\n",
    "                If int, random_state is the seed used by the random number generator; \n",
    "                If RandomState instance, random_state is the random number generator; \n",
    "                If None, the random number generator is the RandomState instance used by np.random.\n",
    "        \n",
    "        Reference\n",
    "        ---------\n",
    "            For more information, please visit https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html\n",
    "        \"\"\"\n",
    "\n",
    "        super(IsolationForest, self).__init__(n_estimators=n_estimators, max_samples=max_samples, \n",
    "            contamination=contamination, **kwargs)\n",
    "\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Auguments\n",
    "        ---------\n",
    "            X: ndarray, the event count matrix of shape num_instances-by-num_events\n",
    "        \"\"\"\n",
    "\n",
    "        print('====== Model summary ======')\n",
    "        super(IsolationForest, self).fit(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict anomalies with mined invariants\n",
    "        Arguments\n",
    "        ---------\n",
    "            X: the input event count matrix\n",
    "        Returns\n",
    "        -------\n",
    "            y_pred: ndarray, the predicted label vector of shape (num_instances,)\n",
    "        \"\"\"\n",
    "        \n",
    "        y_pred = super(IsolationForest, self).predict(X)\n",
    "        y_pred = np.where(y_pred > 0, 0, 1)\n",
    "        return y_pred\n",
    "\n",
    "    def evaluate(self, X, y_true):\n",
    "        print('====== Evaluation summary ======')\n",
    "        y_pred = self.predict(X)\n",
    "        precision, recall, f1 = metrics(y_pred, y_true)\n",
    "        print('Precision: {:.3f}, recall: {:.3f}, F1-measure: {:.3f}\\n'.format(precision, recall, f1))\n",
    "        return precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7461c6f",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1fa621c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Input data summary ======\n",
      "Loading C:/Users/Ubaid/Downloads/example_parsed/HDFS_100k.log_structured.csv\n",
      "[list(['E5', 'E22', 'E5', 'E5', 'E11', 'E11', 'E9', 'E9', 'E11', 'E9', 'E26', 'E26', 'E26', 'E6', 'E5', 'E16', 'E6', 'E5', 'E18', 'E25', 'E26', 'E26', 'E3', 'E25', 'E6', 'E6', 'E5', 'E5', 'E16', 'E18', 'E26', 'E26', 'E5', 'E6', 'E5', 'E16', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E18', 'E25', 'E6', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E26', 'E26', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E25', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E18', 'E6', 'E5', 'E3', 'E3', 'E3', 'E3', 'E3', 'E16', 'E3', 'E3', 'E3', 'E3', 'E26', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3'])\n",
      " list(['E5', 'E5', 'E22', 'E5', 'E11', 'E9', 'E11', 'E9', 'E11', 'E9', 'E26', 'E26', 'E26', 'E3', 'E2', 'E2'])\n",
      " list(['E5', 'E22', 'E5', 'E5', 'E11', 'E9', 'E11', 'E9', 'E11', 'E9', 'E3', 'E26', 'E26', 'E26', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3'])\n",
      " ...\n",
      " list(['E22', 'E5', 'E5', 'E5', 'E26', 'E26', 'E11', 'E9', 'E11', 'E9', 'E11', 'E9', 'E26'])\n",
      " list(['E22', 'E5', 'E5', 'E5', 'E26', 'E26', 'E11', 'E9', 'E11', 'E9', 'E11', 'E9', 'E26'])\n",
      " list(['E22', 'E5', 'E5', 'E5', 'E26', 'E26', 'E11', 'E9', 'E11', 'E9', 'E26', 'E11', 'E9'])] [0 0 1 ... 0 0 1]\n",
      "156 157\n",
      "Total: 7940 instances, 313 anomaly, 7627 normal\n",
      "Train: 3969 instances, 156 anomaly, 3813 normal\n",
      "Test: 3971 instances, 157 anomaly, 3814 normal\n",
      "\n",
      "====== Transformed train data summary ======\n",
      "Train data shape: 3969-by-14\n",
      "\n",
      "====== Transformed test data summary ======\n",
      "Test data shape: 3971-by-14\n",
      "\n",
      "====== Model summary ======\n",
      "Train validation:\n",
      "====== Evaluation summary ======\n",
      "Precision: 1.000, recall: 0.359, F1-measure: 0.528\n",
      "\n",
      "Test validation:\n",
      "====== Evaluation summary ======\n",
      "Precision: 0.985, recall: 0.427, F1-measure: 0.596\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "struct_log = 'C:/Users/Ubaid/Downloads/example_parsed/HDFS_100k.log_structured.csv' # The structured log file\n",
    "label_file = 'C:/Users/Ubaid/Downloads/example_parsed/anomaly_label.csv' # The anomaly label file\n",
    "anomaly_ratio = 0.03 # Estimate the ratio of anomaly samples in the data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    (x_train, y_train), (x_test, y_test) = load_HDFS(struct_log,\n",
    "                                                                label_file=label_file,\n",
    "                                                                window='session', \n",
    "                                                                train_ratio=0.5,\n",
    "                                                                split_type='uniform')\n",
    "    feature_extractor = FeatureExtractor()\n",
    "    x_train = feature_extractor.fit_transform(x_train)\n",
    "    x_test = feature_extractor.transform(x_test)\n",
    "\n",
    "    model = IsolationForest(contamination=anomaly_ratio)\n",
    "    model.fit(x_train)\n",
    "    predicted = model.predict(x_test)\n",
    "    print('Train validation:')\n",
    "    precision, recall, f1 = model.evaluate(x_train, y_train)\n",
    "    \n",
    "    print('Test validation:')\n",
    "    precision, recall, f1 = model.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1947de12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3969, 14)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68095c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3813    1]\n",
      " [  90   67]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      3814\n",
      "           1       0.99      0.43      0.60       157\n",
      "\n",
      "    accuracy                           0.98      3971\n",
      "   macro avg       0.98      0.71      0.79      3971\n",
      "weighted avg       0.98      0.98      0.97      3971\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, recall_score\n",
    "\n",
    "print(confusion_matrix(y_test,predicted))\n",
    "print(classification_report(y_test,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6dbd1dc0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Input data summary ======\n",
      "Loading C:/Users/Ubaid/Downloads/example_parsed/HDFS_2k.log_structured.csv\n",
      "[list(['dc2c74b7']) list(['dc2c74b7']) list(['5d5de21c']) ...\n",
      " list(['09a53393']) list(['dc2c74b7']) list(['09a53393'])] [0 0 0 ... 0 0 0]\n",
      "34 35\n",
      "Total: 2200 instances, 69 anomaly, 2131 normal\n",
      "Train: 1099 instances, 34 anomaly, 1065 normal\n",
      "Test: 1101 instances, 35 anomaly, 1066 normal\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "# from loglizer import dataloader\n",
    "# from loglizer.models import DeepLog\n",
    "# from loglizer.preprocessing import Vectorizer, Iterator\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "hidden_size = 32\n",
    "num_directions = 2\n",
    "topk = 5\n",
    "train_ratio = 0.2\n",
    "window_size = 10\n",
    "epoches = 2\n",
    "num_workers = 2\n",
    "device = 0 \n",
    "\n",
    "struct_log = \"C:/Users/Ubaid/Downloads/example_parsed/HDFS_2k.log_structured.csv\" # The structured log file\n",
    "label_file = 'C:/Users/Ubaid/Downloads/example_parsed/anomaly_label.csv' # The anomaly label file\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    (x_train, y_train), (x_test, y_test) = load_HDFS(struct_log,\n",
    "                                                                label_file=label_file,\n",
    "                                                                window='session', \n",
    "                                                                train_ratio=0.5,\n",
    "                                                                split_type='uniform')\n",
    "#     feature_extractor = FeatureExtractor()\n",
    "#     x_train = feature_extractor.fit_transform(x_train, term_weighting='tf-idf')\n",
    "#     x_test = feature_extractor.transform(x_test)\n",
    "\n",
    "#     model = LogClustering(max_dist=0.3, anomaly_threshold=0.3)\n",
    "#     model.fit(x_train[y_train == 0, :]) # Use only normal samples for training\n",
    "\n",
    "#     print('Train validation:')\n",
    "#     precision, recall, f1 = model.evaluate(x_train, y_train)\n",
    "    \n",
    "#     print('Test validation:')\n",
    "#     precision, recall, f1 = model.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db7245ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['5d5de21c']), list(['d63ef163']), list(['dc2c74b7']),\n",
       "       list(['dba996ef']), list(['dc2c74b7']), list(['09a53393']),\n",
       "       list(['dba996ef']), list(['09a53393']), list(['dc2c74b7']),\n",
       "       list(['5d5de21c']), list(['5d5de21c']), list(['81cee340']),\n",
       "       list(['5d5de21c']), list(['dba996ef']), list(['e3df2680']),\n",
       "       list(['e3df2680']), list(['32777b38']), list(['d63ef163']),\n",
       "       list(['e3df2680']), list(['dba996ef']), list(['5d5de21c']),\n",
       "       list(['81cee340']), list(['d63ef163']), list(['dba996ef']),\n",
       "       list(['5d5de21c']), list(['5d5de21c']), list(['e3df2680']),\n",
       "       list(['09a53393']), list(['dc2c74b7']), list(['e3df2680']),\n",
       "       list(['d63ef163']), list(['dc2c74b7']), list(['81cee340']),\n",
       "       list(['09a53393']), list(['5d5de21c']), list(['e3df2680']),\n",
       "       list(['3d91fa85']), list(['d63ef163']), list(['dc2c74b7']),\n",
       "       list(['dc2c74b7']), list(['32777b38']), list(['d63ef163']),\n",
       "       list(['dc2c74b7']), list(['e3df2680']), list(['d63ef163']),\n",
       "       list(['d63ef163']), list(['dba996ef']), list(['81cee340']),\n",
       "       list(['3d91fa85']), list(['dba996ef']), list(['5d5de21c']),\n",
       "       list(['5d5de21c']), list(['d63ef163']), list(['5d5de21c']),\n",
       "       list(['32777b38']), list(['626085d5']), list(['e3df2680']),\n",
       "       list(['e3df2680']), list(['dba996ef']), list(['e3df2680']),\n",
       "       list(['81cee340']), list(['09a53393']), list(['09a53393']),\n",
       "       list(['e3df2680']), list(['d63ef163']), list(['dba996ef']),\n",
       "       list(['dba996ef']), list(['e3df2680']), list(['09a53393']),\n",
       "       list(['dba996ef']), list(['09a53393']), list(['e3df2680']),\n",
       "       list(['81cee340']), list(['09a53393']), list(['d63ef163']),\n",
       "       list(['e3df2680']), list(['09a53393']), list(['dba996ef']),\n",
       "       list(['5d5de21c']), list(['09a53393']), list(['d63ef163']),\n",
       "       list(['5d5de21c']), list(['5d5de21c']), list(['09a53393']),\n",
       "       list(['5d5de21c']), list(['626085d5']), list(['dc2c74b7']),\n",
       "       list(['626085d5']), list(['dba996ef']), list(['e3df2680']),\n",
       "       list(['81cee340']), list(['dc2c74b7']), list(['626085d5']),\n",
       "       list(['d63ef163']), list(['dba996ef']), list(['e3df2680']),\n",
       "       list(['dc2c74b7']), list(['5d5de21c']), list(['e3df2680']),\n",
       "       list(['e3df2680'])], dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd51a97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
